---
title: "Practical Machine Learning course project"
author: "Kevin Hatala"
date: "Sunday, April 26, 2015"
output: html_document
---

In this assignment, our goal was to use data from accelerometers to predict the manners in which six individuals performed barbell lifts.

My R script for this project depends upon the caret, dplyr, rattle, and randomForest packages, which are all loaded here in advance in the R Markdown version of my script (rather than appearing in-line with the code).

```{r load_packages, include=FALSE}
library(caret)
library(dplyr)
library(rattle)
library(randomForest)
```

To start this project, I began by reading the description of the data set. In reading this document, I immediately realized that there were some adjustments I should make to the training data set before trying to build a predictive model. Specifically, the data set includes several 'summary' variables that are not calculated for every accelerometer reading (every row) but rather are calculated over the course of a certain 'time window' (several consecutive rows). In the testing set, we are simply provided with discrete single accelerometer measurements, and thus the 'summary' variables are all equal to NA. Therefore, these variables will not be uesful for prediction. After loading the training and testing data sets into R, I removed all 'summary' variables from the training set using the select function from the dplyr package, following the code below.
```{r,include=FALSE}
training<-read.csv("~\\data\\pml-training.csv")
testing<-read.csv("~\\data\\pml-testing.csv")
```
```{r}
training<-select(training,-contains("avg"))
training<-select(training,-contains("stddev"))
training<-select(training,-contains("var"))
training<-select(training,-contains("max"))
training<-select(training,-contains("min"))
training<-select(training,-contains("amplitude"))
training<-select(training,-contains("skewness"))
training<-select(training,-contains("kurtosis"))
```
I also excluded the user ID, timestamp, and window number variables since none of these are particularly useful for building a model that can predict activities based on accelerometer readings alone.
```{r}
training<-select(training,8:60)
```

Next, I noticed the large size of the training data set - 19622 observations - and decided that this was sufficiently large that I should split the training data set into a training group and a validation group.
```{r}
inTrain<-createDataPartition(y=training$classe,p=0.7,list=FALSE)
train<-training[inTrain,]
valid<-training[-inTrain,]
```

At this point, the training data set consisted of 53 variables. I decided to test whether any of those 53 covariates were of near zero variance, and therefore may not be useful to include in a predictive model.
```{r}
nzv<-nearZeroVar(train,saveMetrics=TRUE)
```
None of these 53 variables had near zero variance, so all were kept for use in the predictive model. I examined the correlations between these remaining 53 variables though, to determine whether or not any were highly correlated and the number of covariates could be reduced.
```{r}
correlations<-abs(cor(train[,-53]))
diag(correlations)<-0
corVars<-which(correlations>0.8,arr.ind=T)
length(corVars)
```
I found that 76 of the correlations between different accelerometer measurements were greater than 0.8, suggesting that the number of covariates could be substantially reduced by preprocessing with principal components analysis.

I pre-processed the training data set by first centering and scaling the data (such that certain variables measured on the largest scales did not unduly influence the PCA results) and then performing a principal components analysis. I decided to retain the components that together described 99% of the variance in the data set, in hopes that the predictive model could achieve high accuracy. I built a new training data set that consisted of just those PC scores and the activity classes. This new data set consisted of 36 predictors (scores on 36 PC axes) and 1 response. I used the formula built on the training set to pre-process the validation set in the same way.
```{r}
preProc<-preProcess(train[,-53],method=c("center","scale","pca"),thresh=0.99)
trainProc<-predict(preProc,train[,-53])
trainProc<-mutate(trainProc,classe=train$classe)
validProc<-predict(preProc,valid[,-53])
validProc<-mutate(validProc,classe=valid$classe)
```

With my data pre-processed, I was ready to begin testing predictive models. In my first attempt, I built a classification tree (with the "rpart" method) to predict classe from all 36 pre-processed PC scores. In examining the dendrogram for this tree though, I realized that the leaves were relatively 'impure' and that certain classes weren't even predicted by the classification tree. A confusion matrix revealed that predictive accuracy was less than 1% within the training set. 

```{r,include=FALSE}
treeFit<-train(classe~.,method="rpart",data=trainProc)
```
```{r, echo=FALSE}
fancyRpartPlot(treeFit$finalModel)
```
```{r}
confusionMatrix(predict(treeFit,trainProc[,-37]),trainProc$classe)$overall
```

Next I tried a random forests model to predict classe from the 36 pre-processed PC scores. The confusion matrix based on out-of-bag data revealed that, within the training set, classification errors for classes A-E were 4% or less. This was a remarkable improvement over the classification tree.

```{r}
set.seed(3733)
rfFit<-randomForest(classe~.,data=trainProc,importance=TRUE)
rfFit$confusion
```

Before trying to use the random forets model for prediction though, I decided to attempt one final model fit using boosting. I used the "gbm" method to build a boosted tree model. I found that the boosted trees each had accuracies ranging from 62-85% within the training set. So it appeared that none of these alone would outperform the random forests model, although I considered it possible that the boosted model could outperform the random forests model when its predictive abilities were tested on the validation set.

```{r}
boostFit<-train(classe~.,data=trainProc,method="gbm",verbose=FALSE)
print(boostFit)
```

I used both the random forests and boosted trees models to generate predictions on the validation set, and see which performed better. I created confusion matrices for each set of predictions in order to gain an estimate of out-of-sample prediction error for each of the two models.
```{r}
rfPreds<-predict(rfFit,newdata=validProc[,-37])
confusionMatrix(rfPreds,validProc$classe)
boostPreds<-predict(boostFit,newdata=validProc[,-37])
confusionMatrix(boostPreds,validProc$classe)
```

The random forests model out-performed the boosted trees model when predicting on the validation set. The accuracy of predictions from the random forests model was about 98%, while that of the boosted trees model was only about 85%. So I selected the random forests model as the best model to use in predicting the classes of the test data set, and expected its out-of-sample error rate to be about 2%. 

All of the same transformations and pre-processing functions were applied to the testing data set as had been performed on the training data set. Then predictions were generated for all 20 test cases.

```{r}
testing<-select(testing,-contains("avg"))
testing<-select(testing,-contains("stddev"))
testing<-select(testing,-contains("var"))
testing<-select(testing,-contains("max"))
testing<-select(testing,-contains("min"))
testing<-select(testing,-contains("amplitude"))
testing<-select(testing,-contains("skewness"))
testing<-select(testing,-contains("kurtosis"))
testing<-select(testing,8:60)
testProc<-predict(preProc,testing[,-53])
testProc<-mutate(testProc,problem_id=testing$problem_id)
predictions<-predict(rfFit,newdata=testProc[,-37])
```

The resulting predictions were uploaded to Coursera and it was determined that the random forests model generated here was able to predict the 20 test cases with 100% accuracy.
